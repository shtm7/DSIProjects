# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 3: Sharing Dataset on Kaggle

### Description

We have been using various datasets in the course that are either model/toy datasets or collected in conditions fairly remote to local relevance. With the newfound API and webscraping skills that you learned, this project challenges you to create a dataset revolving around a topic, problem, or theme of your choice, clean it, properly document it, and submit it to the Kaggle dataset repository. Curating and sharing a dataset is an integral part of your skills and practice as a data scientist that should not be overlooked!

For project 3, which will be individual work, your goal is threefold:

1. Define a domain, issue, and problem that you are interested in (preferably with local/regional relevance aslo you should have a ML algorithm in mind).
2. Collect, clean, and submit the dataset to the Kaggle datasets.
3. Submit the data collection and a starter kernel in public associated with the published dataset.
4. On Kaggle you should get 10 out of 10 for the usability :
. Easy to understand and includes essential metadata
- checkSubtitle
- checkTags
- checkOverview description
- checkCover image
. Rich, machine readable file formats and metadata
- checkFile descriptions
- checkColumn descriptions
- checkLicense specified
- checkAcceptable file formats used
. Assurances the dataset is maintained
- checkProvenance is specified
- checkUpdate frequency is specified
- checkHas a public kernel or task
- Add your data dictionary and problem ptatement > what ML models can be solved using your datasets.
5. you should also add to your dataset link to your Jupyter Notebook

#### Readings


- To help you get started, please read [this blog post](https://medium.com/analytics-vidhya/publishing-your-first-dataset-on-kaggle-6be8c37e59e8)

- For inspiration on how a company successfully published a dataset on Kaggle read [this story](https://towardsdatascience.com/how-we-published-a-successful-dataset-on-kaggle-2945de537597).

- More information and documentation on the Kaggle datasets platform see [this page](https://www.kaggle.com/docs/datasets)
- Read about the Common license types for datasets [here](https://help.data.world/hc/en-us/articles/115006114287-Common-license-types-for-datasets) and [here](https://creativecommons.org/publicdomain/zero/1.0/)


### Requirements

- Gather and prepare your data using API or web scraping. A ready-made dataset is *NOT* allowed. **be careful and save your data as a CSV file or using pickles whenever you scrape a good amount of data so that you do not have to re-run the code every time to get the data which is time and resources consuming**

- Make your data accessible and readable by using common open file formats like CSV.
- Take the time to describe your dataset thoroughly.
- Pick a clear, open license ensuring your dataset is reusable.
- Minimum records or rows should be **1000** after cleaning your data.
- Publish a kernel on your dataset with a simple EDA to help others learn how they can work with the data. The kernel should show features of the dataset with a plot or two to showcase some variables in the dataset. Also, raise potentially interesting questions that could be answered using this dataset.
- As always, you should pull your data collection and cleaning code that is on your work Notebook to project-3 on the Github organization. **Do not forget to add the link to your dataset in Kaggle at your Notebook**

---

### Submission
- Due date:  Sunday 31st January 2021.

